/**
 * AI Message Generation Module
 * 
 * Generates dynamic notification messages using OpenAI-compatible AI endpoints.
 * Supports: Ollama, LM Studio, LocalAI, vLLM, llama.cpp, Jan.ai, etc.
 * 
 * Uses native fetch() - no external dependencies required.
 */

import { getTTSConfig } from './tts.js';

/**
 * Generate a message using an OpenAI-compatible AI endpoint
 * @param {string} promptType - The type of prompt ('idle', 'permission', 'question', 'idleReminder', 'permissionReminder', 'questionReminder')
 * @param {object} context - Optional context about the notification (for future use)
 * @returns {Promise<string|null>} Generated message or null if failed
 */
export async function generateAIMessage(promptType, context = {}) {
  const config = getTTSConfig();
  
  // Check if AI messages are enabled
  if (!config.enableAIMessages) {
    return null;
  }
  
  // Get the prompt for this type
  const prompt = config.aiPrompts?.[promptType];
  if (!prompt) {
    console.error(`[AI Messages] No prompt configured for type: ${promptType}`);
    return null;
  }
  
  try {
    // Build headers
    const headers = { 'Content-Type': 'application/json' };
    if (config.aiApiKey) {
      headers['Authorization'] = `Bearer ${config.aiApiKey}`;
    }
    
    // Build endpoint URL (ensure it ends with /chat/completions)
    let endpoint = config.aiEndpoint || 'http://localhost:11434/v1';
    if (!endpoint.endsWith('/chat/completions')) {
      endpoint = endpoint.replace(/\/$/, '') + '/chat/completions';
    }
    
    // Create abort controller for timeout
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), config.aiTimeout || 15000);
    
    // Make the request
    const response = await fetch(endpoint, {
      method: 'POST',
      headers,
      signal: controller.signal,
      body: JSON.stringify({
        model: config.aiModel || 'llama3',
        messages: [
          {
            role: 'system',
            content: 'You are a helpful assistant that generates short notification messages. Output only the message text, nothing else. No quotes, no explanations.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 1000,  // High value to accommodate thinking models (e.g., Gemini 2.5) that use internal reasoning tokens
        temperature: 0.7
      })
    });
    
    clearTimeout(timeout);
    
    if (!response.ok) {
      const errorText = await response.text().catch(() => 'Unknown error');
      console.error(`[AI Messages] API error ${response.status}: ${errorText}`);
      return null;
    }
    
    const data = await response.json();
    
    // Extract the message content
    const message = data.choices?.[0]?.message?.content?.trim();
    
    if (!message) {
      console.error('[AI Messages] Empty response from AI');
      return null;
    }
    
    // Clean up the message (remove quotes if AI added them)
    let cleanMessage = message.replace(/^["']|["']$/g, '').trim();
    
    // Validate message length (sanity check)
    if (cleanMessage.length < 5 || cleanMessage.length > 200) {
      console.error(`[AI Messages] Message length invalid: ${cleanMessage.length} chars`);
      return null;
    }
    
    return cleanMessage;
    
  } catch (error) {
    if (error.name === 'AbortError') {
      console.error(`[AI Messages] Request timed out after ${config.aiTimeout || 15000}ms`);
    } else {
      console.error(`[AI Messages] Error: ${error.message}`);
    }
    return null;
  }
}

/**
 * Get a smart message - tries AI first, falls back to static messages
 * @param {string} eventType - 'idle', 'permission', 'question'
 * @param {boolean} isReminder - Whether this is a reminder message
 * @param {string[]} staticMessages - Array of static fallback messages
 * @param {object} context - Optional context (e.g., { count: 3 } for batched notifications)
 * @returns {Promise<string>} The message to speak
 */
export async function getSmartMessage(eventType, isReminder, staticMessages, context = {}) {
  const config = getTTSConfig();
  
  // Determine the prompt type
  const promptType = isReminder ? `${eventType}Reminder` : eventType;
  
  // Try AI generation if enabled
  if (config.enableAIMessages) {
    try {
      const aiMessage = await generateAIMessage(promptType, context);
      if (aiMessage) {
        // Log success for debugging
        if (config.debugLog) {
          console.log(`[AI Messages] Generated: ${aiMessage}`);
        }
        return aiMessage;
      }
    } catch (error) {
      console.error(`[AI Messages] Generation failed: ${error.message}`);
    }
    
    // Check if fallback is disabled
    if (!config.aiFallbackToStatic) {
      // Return a generic message if fallback disabled and AI failed
      return 'Notification: Please check your screen.';
    }
  }
  
  // Fallback to static messages
  if (!Array.isArray(staticMessages) || staticMessages.length === 0) {
    return 'Notification';
  }
  
  return staticMessages[Math.floor(Math.random() * staticMessages.length)];
}

/**
 * Test connectivity to the AI endpoint
 * @returns {Promise<{success: boolean, message: string, model?: string}>}
 */
export async function testAIConnection() {
  const config = getTTSConfig();
  
  if (!config.enableAIMessages) {
    return { success: false, message: 'AI messages not enabled' };
  }
  
  try {
    const headers = { 'Content-Type': 'application/json' };
    if (config.aiApiKey) {
      headers['Authorization'] = `Bearer ${config.aiApiKey}`;
    }
    
    // Try to list models (simpler endpoint to test connectivity)
    let endpoint = config.aiEndpoint || 'http://localhost:11434/v1';
    endpoint = endpoint.replace(/\/$/, '') + '/models';
    
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 5000);
    
    const response = await fetch(endpoint, {
      method: 'GET',
      headers,
      signal: controller.signal
    });
    
    clearTimeout(timeout);
    
    if (response.ok) {
      const data = await response.json();
      const models = data.data?.map(m => m.id) || [];
      return {
        success: true,
        message: `Connected! Available models: ${models.slice(0, 3).join(', ')}${models.length > 3 ? '...' : ''}`,
        models
      };
    } else {
      return { success: false, message: `HTTP ${response.status}: ${response.statusText}` };
    }
    
  } catch (error) {
    if (error.name === 'AbortError') {
      return { success: false, message: 'Connection timed out' };
    }
    return { success: false, message: error.message };
  }
}

export default { generateAIMessage, getSmartMessage, testAIConnection };
